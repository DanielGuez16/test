#!/usr/bin/env python3
#run.py
"""
Steering ALM Metrics - Version avec Templates
============================================

Application FastAPI utilisant un syst√®me de templates Jinja2
pour s√©parer la logique m√©tier de la pr√©sentation.
"""

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import pandas as pd
import uvicorn
import uuid
import logging
from pathlib import Path
from datetime import datetime
import math
import statistics
import chardet

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cr√©ation de l'application FastAPI
app = FastAPI(title="Steering ALM Metrics", version="2.0.0")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Cr√©ation des dossiers requis
required_dirs = ["data", "templates", "static", "static/js", "static/css", "static/images"]
for directory in required_dirs:
    Path(directory).mkdir(exist_ok=True)

# Formats support√©s
SUPPORTED_EXTENSIONS = ['.xlsx', '.xls', '.xlsm', '.xlsb', '.csv', '.tsv', '.txt']

# Configuration des fichiers statiques et templates
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# Variables globales pour la session (en production: utiliser une base de donn√©es)
file_session = {"files": {}}

REQUIRED_COLUMNS = {
    "balance_sheet": [
        "Top Conso",
        "R√©affectation", 
        "Groupe De Produit",
        "Nominal Value"
    ],
    "consumption": [
        "Top Conso",
        "LCR_ECO_GROUPE_METIERS",
        "LCR_ECO_IMPACT_LCR",
        "M√©tier",
        "Sous-M√©tier"
    ]
}

ALL_REQUIRED_COLUMNS = list(set(
    REQUIRED_COLUMNS["balance_sheet"] + 
    REQUIRED_COLUMNS["consumption"]
))

def convert_any_file_to_excel(file_path):
    """
    Lit n'importe quel fichier et le convertit en Excel - Version optimis√©e
    Retourne le chemin du fichier Excel cr√©√©
    """
    file_path = Path(file_path)
    extension = file_path.suffix.lower()
    
    try:
        # Si c'est d√©j√† Excel, on garde tel quel
        if extension in ['.xlsx', '.xls', '.xlsm', '.xlsb']:
            return file_path
        
        # Si c'est CSV/TSV/TXT, on le convertit
        elif extension in ['.csv', '.tsv', '.txt']:
            logger.info(f"Conversion {extension} vers Excel en cours...")
            
            # D√©tecter l'encodage
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)
                encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'
            
            logger.info(f"Encodage d√©tect√©: {encoding}")
            
            # D√©tecter le d√©limiteur
            if extension == '.tsv':
                delimiter = '\t'
            else:
                with open(file_path, 'r', encoding=encoding) as f:
                    first_line = f.readline()
                    if ';' in first_line:
                        delimiter = ';'
                    elif '\t' in first_line:
                        delimiter = '\t'
                    else:
                        delimiter = ','
            
            logger.info(f"D√©limiteur d√©tect√©: '{delimiter}'")
            
            # Lire le fichier CSV avec optimisations pour √©viter les warnings
            df = pd.read_csv(
                file_path, 
                delimiter=delimiter, 
                encoding=encoding,
                low_memory=False,      # √âvite les warnings DtypeWarning
                dtype=str,             # Force tout en string pour √©viter d√©tection automatique
                na_filter=False        # √âvite la conversion des valeurs vides en NaN
            )
            
            # Nettoyer les noms de colonnes
            df.columns = df.columns.astype(str).str.strip()
            
            logger.info(f"CSV lu: {len(df)} lignes, {len(df.columns)} colonnes")
            
            # Convertir les colonnes num√©riques connues apr√®s lecture
            numeric_columns = ['Nominal Value', 'LCR_ECO_IMPACT_LCR']
            for col in numeric_columns:
                if col in df.columns:
                    # Nettoyer et convertir en num√©rique
                    df[col] = pd.to_numeric(df[col].str.replace(',', '.'), errors='coerce')
            
            # Convertir en Excel
            excel_path = file_path.with_suffix('.xlsx')
            logger.info(f"√âcriture Excel vers: {excel_path}")
            
            df.to_excel(excel_path, index=False, engine='openpyxl')
            
            # Supprimer l'original
            file_path.unlink()
            
            logger.info(f"Conversion termin√©e: {extension} ‚Üí .xlsx")
            return excel_path
        
        else:
            raise ValueError(f"Format non support√©: {extension}")
            
    except Exception as e:
        logger.error(f"Erreur conversion {extension}: {e}")
        raise ValueError(f"Erreur conversion vers Excel: {str(e)}")
    
#######################################################################################################################################

#                           API

#######################################################################################################################################


@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """
    Page d'accueil de l'application
    
    Utilise le template Jinja2 pour s√©parer la pr√©sentation
    de la logique m√©tier.
    """
    try:
        return templates.TemplateResponse("index.html", {
            "request": request,
            "title": "Steering ALM Metrics",
            "version": "2.0.0",
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"Erreur chargement template: {e}")
        # Fallback en cas d'erreur de template
        return HTMLResponse(content="""
            <html>
                <body style="font-family: Arial; padding: 50px; text-align: center;">
                    <h1>Steering ALM Metrics</h1>
                    <p style="color: red;">Erreur de chargement du template</p>
                    <p>V√©rifiez que le fichier templates/index.html existe</p>
                </body>
            </html>
        """)

@app.get("/health")
async def health_check():
    """Endpoint de v√©rification de l'√©tat de l'application"""
    return {
        "status": "healthy",
        "service": "steering-alm-metrics",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "active_files": len(file_session.get("files", {})),
        "templates_available": Path("templates/index.html").exists(),
        "static_available": Path("static/js/main.js").exists()
    }

@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...), file_type: str = Form(...)):
    """
    Upload avec conversion automatique vers Excel
    """
    try:
        logger.info(f"Upload re√ßu: {file.filename}, type: {file_type}")
        
        # Validation du fichier
        if not file.filename:
            raise HTTPException(status_code=400, detail="Nom de fichier manquant")
        
        file_extension = Path(file.filename).suffix.lower()
        SUPPORTED_EXTENSIONS = ['.xlsx', '.xls', '.xlsm', '.xlsb', '.csv', '.tsv', '.txt']
        
        if file_extension not in SUPPORTED_EXTENSIONS:
            raise HTTPException(
                status_code=400, 
                detail=f"Format non support√©: {file_extension}. Formats accept√©s: {', '.join(SUPPORTED_EXTENSIONS)}"
            )

        # Sauvegarder le fichier
        contents = await file.read()
        unique_filename = f"{file_type}_{uuid.uuid4().hex[:8]}_{file.filename}"
        file_path = Path("data") / unique_filename
        
        with open(file_path, "wb") as f:
            f.write(contents)
        
        # CONVERSION AUTOMATIQUE VERS EXCEL
        try:
            excel_path = convert_any_file_to_excel(file_path)
            logger.info(f"Fichier converti vers Excel: {excel_path}")
            
            # Lire le fichier Excel pour validation
            df = pd.read_excel(excel_path, engine='openpyxl')
            df.columns = df.columns.astype(str).str.strip()
            
        except Exception as e:
            file_path.unlink(missing_ok=True)
            raise HTTPException(status_code=422, detail=f"Erreur conversion: {str(e)}")
        
        # V√©rifier les colonnes
        missing_columns = [col for col in ALL_REQUIRED_COLUMNS if col not in df.columns]
        
        # Stocker les infos (avec le nom du fichier Excel)
        file_session["files"][file_type] = {
            "filename": excel_path.name,  # Nom du fichier Excel converti
            "original_name": file.filename,
            "original_format": file_extension,
            "rows": len(df),
            "columns": len(df.columns),
            "upload_time": datetime.now().isoformat(),
            "missing_columns": missing_columns
        }
        
        return {
            "success": True,
            "message": f"Fichier {file_type} lu et converti en Excel ({file_extension} ‚Üí .xlsx)",
            "filename": file.filename,
            "original_format": file_extension,
            "converted_to": "Excel",
            "rows": len(df),
            "columns": len(df.columns),
            "missing_columns": missing_columns,
            "file_size": len(contents)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur upload: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")
    
@app.post("/api/analyze")
async def analyze_files():
    """
    Endpoint d'analyse des fichiers LCR - Version compl√®te avec Balance Sheet et Consumption
    """
    try:
        logger.info("üîç D√©but de l'analyse Balance Sheet + Consumption")
        
        # V√©rification de la pr√©sence des deux fichiers
        if len(file_session.get("files", {})) < 2:
            raise HTTPException(
                status_code=400, 
                detail="Les deux fichiers (J et J-1) sont requis pour l'analyse"
            )
        
        if "j" not in file_session["files"] or "jMinus1" not in file_session["files"]:
            raise HTTPException(
                status_code=400,
                detail="Fichiers manquants. Veuillez uploader le fichier J et le fichier J-1"
            )
        
        # Chargement des fichiers
        dataframes = {}
        for file_type, file_info in file_session["files"].items():
            file_path = Path("data") / file_info["filename"]
            
            if not file_path.exists():
                raise HTTPException(
                    status_code=404,
                    detail=f"Fichier {file_type} non trouv√© sur le serveur"
                )
            
            df = pd.read_excel(file_path, engine='openpyxl')
            df.columns = df.columns.astype(str).str.strip()
            dataframes[file_type] = df
            
            logger.info(f"üìä {file_type}: {len(df)} lignes charg√©es")
        
        # G√©n√©ration du tableau crois√© dynamique Balance Sheet
        balance_sheet_results = create_balance_sheet_pivot_table(dataframes)
        
        # G√©n√©ration de l'analyse Consumption
        consumption_results = create_consumption_analysis_grouped_only(dataframes)
        
        logger.info("‚úÖ Analyses Balance Sheet et Consumption termin√©es avec succ√®s")
        
        return {
            "success": True,
            "message": "Analyses Balance Sheet et Consumption termin√©es",
            "timestamp": datetime.now().isoformat(),
            "results": {
                "balance_sheet": balance_sheet_results,
                "consumption": consumption_results
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur d'analyse: {str(e)}")

#######################################################################################################################################

#                           BALANCE SHEET

#######################################################################################################################################

def create_balance_sheet_pivot_table(dataframes):
    """
    Cr√©e le tableau crois√© dynamique Balance Sheet
    
    Args:
        dataframes: Dict contenant les DataFrames 'j' et 'jMinus1'
    
    Returns:
        Dict contenant les r√©sultats de l'analyse
    """
    try:
        logger.info("üíº Cr√©ation du TCD Balance Sheet")
        
        pivot_tables = {}
        totals_summary = {}
        
        # Traitement de chaque fichier
        for file_type, df in dataframes.items():
            logger.info(f"üîÑ Traitement du fichier {file_type}")
            
            # Filtrage des donn√©es
            df_filtered = df[df["Top Conso"] == "O"].copy()
            logger.info(f"üìã Apr√®s filtrage Top Conso='O': {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e avec Top Conso='O' pour {file_type}")
                continue
            
            # Pr√©paration des colonnes
            df_filtered["Nominal Value"] = pd.to_numeric(
                df_filtered["Nominal Value"], errors='coerce'
            ).fillna(0)
            
            df_filtered["R√©affectation"] = df_filtered["R√©affectation"].astype(str).str.upper().str.strip()
            
            # Filtrage ACTIF/PASSIF uniquement
            df_filtered = df_filtered[
                df_filtered["R√©affectation"].isin(["ACTIF", "PASSIF"])
            ].copy()
            
            logger.info(f"üìä Apr√®s filtrage ACTIF/PASSIF: {len(df_filtered)} lignes")
            logger.info(f"üè∑Ô∏è R√©affectations trouv√©es: {sorted(df_filtered['R√©affectation'].unique())}")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e ACTIF/PASSIF pour {file_type}")
                continue
            
            # Cr√©ation du tableau crois√© dynamique
            pivot_table = pd.pivot_table(
                df_filtered,
                index="Groupe De Produit",
                columns="R√©affectation",
                values="Nominal Value",
                aggfunc="sum",
                fill_value=0,
                margins=True,
                margins_name="TOTAL"
            )
            
            # Conversion en milliards d'euros
            pivot_table = (pivot_table / 1_000_000_000).round(2)
            
            # Assurer la pr√©sence des colonnes ACTIF et PASSIF
            for col in ["ACTIF", "PASSIF"]:
                if col not in pivot_table.columns:
                    pivot_table[col] = 0.0
            
            # R√©organisation des colonnes
            pivot_table = pivot_table[["ACTIF", "PASSIF"]]
            
            pivot_tables[file_type] = pivot_table
            
            # Calcul des totaux
            if "TOTAL" in pivot_table.index:
                totals_summary[file_type] = {
                    "ACTIF": float(pivot_table.loc["TOTAL", "ACTIF"]),
                    "PASSIF": float(pivot_table.loc["TOTAL", "PASSIF"])
                }
            
            logger.info(f"‚úÖ TCD {file_type} cr√©√©: {pivot_table.shape[0]} lignes x {pivot_table.shape[1]} colonnes")
        
        # G√©n√©ration du HTML du tableau combin√©
        pivot_html = generate_pivot_table_html(pivot_tables)
        
        # Calcul des variations
        variations = calculate_variations(totals_summary)
        
        # G√©n√©ration du r√©sum√© ex√©cutif
        summary = generate_executive_summary(variations)
        
        return {
            "title": "Balance Sheet",
            "pivot_table_html": pivot_html,
            "variations": variations,
            "summary": summary,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filters_applied": {
                    "top_conso": "O",
                    "reaffectation": ["ACTIF", "PASSIF"]
                },
                "files_analyzed": list(pivot_tables.keys())
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation TCD: {e}")
        return {
            "title": "Balance Sheet - Erreur",
            "error": str(e),
            "pivot_table_html": "<p class='text-danger'>Erreur lors de la g√©n√©ration du tableau</p>"
        }

def generate_pivot_table_html(pivot_tables):
    """
    G√©n√®re le HTML du tableau crois√© dynamique combin√©
    
    Args:
        pivot_tables: Dict des tables pivot par type de fichier
    
    Returns:
        String HTML du tableau format√©
    """
    if len(pivot_tables) < 2:
        return "<div class='alert alert-warning'>Donn√©es insuffisantes pour g√©n√©rer le TCD complet</div>"
    
    pivot_j = pivot_tables.get("j")
    pivot_j1 = pivot_tables.get("jMinus1")
    
    if pivot_j is None or pivot_j1 is None:
        return "<div class='alert alert-danger'>Erreur: donn√©es manquantes pour la comparaison</div>"
    
    # Liste des groupes de produits (sans TOTAL pour l'instant)
    all_products = sorted([p for p in set(pivot_j.index) | set(pivot_j1.index) if p != "TOTAL"])
    all_products.append("TOTAL")  # Ajouter TOTAL √† la fin
    
    html = """
    <table class="table table-bordered pivot-table">
        <thead>
            <tr>
                <th rowspan="2" class="align-middle">Groupe De Produit</th>
                <th colspan="2" class="text-center header-j-minus-1">D-1 (Yesterday)</th>
                <th colspan="2" class="text-center header-j">D (Today)</th>
                <th colspan="2" class="text-center header-variation">Variation (D - D-1)</th>
            </tr>
            <tr>
                <th class="text-center header-j-minus-1">ACTIF</th>
                <th class="text-center header-j-minus-1">PASSIF</th>
                <th class="text-center header-j">ACTIF</th>
                <th class="text-center header-j">PASSIF</th>
                <th class="text-center header-variation">ACTIF</th>
                <th class="text-center header-variation">PASSIF</th>
            </tr>
        </thead>
        <tbody>
    """
    
    # G√©n√©ration des lignes
    for product in all_products:
        css_class = "total-row" if product == "TOTAL" else ""
        html += f'<tr class="{css_class}">'
        html += f'<td class="fw-bold">{product}</td>'
        
        # Valeurs J-1
        for category in ["ACTIF", "PASSIF"]:
            value_j1 = pivot_j1.loc[product, category] if product in pivot_j1.index else 0.0
            html += f'<td class="text-end numeric-value">{value_j1:.2f}</td>'
        
        # Valeurs J
        for category in ["ACTIF", "PASSIF"]:
            value_j = pivot_j.loc[product, category] if product in pivot_j.index else 0.0
            html += f'<td class="text-end numeric-value">{value_j:.2f}</td>'
        
        # Variations
        for category in ["ACTIF", "PASSIF"]:
            value_j1 = pivot_j1.loc[product, category] if product in pivot_j1.index else 0.0
            value_j = pivot_j.loc[product, category] if product in pivot_j.index else 0.0
            variation = value_j - value_j1
            
            css_class = "variation-positive" if variation > 0 else "variation-negative" if variation < 0 else ""
            sign = "+" if variation > 0 else ""
            html += f'<td class="text-end numeric-value {css_class}">{sign}{variation:.2f}</td>'
        
        html += '</tr>'
    
    html += """
        </tbody>
    </table>
    """
    
    return html

def calculate_variations(totals_summary):
    """
    Calcule les variations entre J et J-1
    
    Args:
        totals_summary: Dict des totaux par fichier
    
    Returns:
        Dict des variations calcul√©es
    """
    if "j" not in totals_summary or "jMinus1" not in totals_summary:
        return {}
    
    variations = {}
    for category in ["ACTIF", "PASSIF"]:
        j_value = totals_summary["j"].get(category, 0)
        j1_value = totals_summary["jMinus1"].get(category, 0)
        
        variations[category] = {
            "j_minus_1": round(j1_value, 2),
            "j": round(j_value, 2),
            "variation": round(j_value - j1_value, 2)
        }
    
    return variations

def generate_executive_summary(variations):
    """
    G√©n√®re un r√©sum√© ex√©cutif de l'analyse
    
    Args:
        variations: Dict des variations calcul√©es
    
    Returns:
        String du r√©sum√© ex√©cutif
    """
    if not variations:
        return "Analyse incompl√®te - donn√©es insuffisantes pour g√©n√©rer un r√©sum√©."
    
    date_str = datetime.now().strftime("%d/%m/%Y")
    summary_parts = []
    
    for category, data in variations.items():
        variation = data["variation"]
        if abs(variation) >= 0.1:  # Variations significatives >= 100M‚Ç¨
            direction = "hausse" if variation > 0 else "baisse"
            summary_parts.append(f"{category}: {direction} de {abs(variation):.2f} Md‚Ç¨")
    
    if summary_parts:
        return f"On {date_str} Natixis' balance sheet presents some variations: {', '.join(summary_parts)}."
    else:
        return f"Balance Sheet au {date_str} - Variations mineures observ√©es (< 100M‚Ç¨)."

#######################################################################################################################################

#                           CONSUMPTION

#######################################################################################################################################

def create_consumption_analysis_grouped_only(dataframes):
    """
    Cr√©e l'analyse Consumption UNIQUEMENT par Groupe M√©tiers (sans d√©tail des m√©tiers)
    """
    try:
        logger.info("üíº Cr√©ation de l'analyse Consumption - Groupes M√©tiers uniquement")
        
        consumption_grouped = {}
        totals_by_group = {}
        metier_details = {}  # Initialiser au d√©but
        
        # Traitement de chaque fichier
        for file_type, df in dataframes.items():
            logger.info(f"üîÑ Traitement Consumption group√© pour {file_type}")
            
            # V√©rification des colonnes requises
            required_cols = ["Top Conso", "LCR_ECO_GROUPE_METIERS", "LCR_ECO_IMPACT_LCR"]
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour Consumption {file_type}: {missing_cols}")
                continue
            
            # Filtrage des donn√©es (Top Conso = "O")
            df_filtered = df[df["Top Conso"] == "O"].copy()
            logger.info(f"üìã Apr√®s filtrage Top Conso='O': {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e avec Top Conso='O' pour Consumption {file_type}")
                continue
            
            # Pr√©paration des donn√©es
            df_filtered["LCR_ECO_IMPACT_LCR"] = pd.to_numeric(
                df_filtered["LCR_ECO_IMPACT_LCR"], errors='coerce'
            ).fillna(0)
            
            # Nettoyage des champs texte
            df_filtered["LCR_ECO_GROUPE_METIERS"] = df_filtered["LCR_ECO_GROUPE_METIERS"].astype(str).str.strip()
            
            # Groupement UNIQUEMENT par LCR_ECO_GROUPE_METIERS
            grouped = df_filtered.groupby("LCR_ECO_GROUPE_METIERS")["LCR_ECO_IMPACT_LCR"].sum().reset_index()
            
            # Conversion en milliards
            grouped["LCR_ECO_IMPACT_LCR_Bn"] = (grouped["LCR_ECO_IMPACT_LCR"] / 1_000_000_000).round(3)
            
            consumption_grouped[file_type] = grouped
            
            # Calcul des totaux
            total_global = (df_filtered["LCR_ECO_IMPACT_LCR"].sum() / 1_000_000_000).round(3)
            
            totals_by_group[file_type] = {
                "total_global": total_global,
                "by_groupe_metiers": grouped.set_index("LCR_ECO_GROUPE_METIERS")["LCR_ECO_IMPACT_LCR_Bn"].to_dict(),
                "grouped_data": grouped
            }
            
            logger.info(f"‚úÖ Consumption group√© {file_type}: {len(grouped)} groupes, Total global = {total_global:.3f} Bn")
        
        # G√©n√©ration du HTML du tableau group√©
        consumption_html = generate_consumption_grouped_table_html(consumption_grouped)
        
        # Calcul des variations
        variations = calculate_consumption_grouped_variations(totals_by_group)

        # G√©n√©ration de l'analyse textuelle
        analysis_text, significant_groups = generate_consumption_grouped_analysis_text(variations, totals_by_group, dataframes)

        # G√©n√©ration de l'analyse d√©taill√©e par m√©tier (NOUVELLE VERSION)
        metier_detailed_analysis = generate_metier_detailed_analysis(significant_groups, dataframes)

        # Pr√©parer les donn√©es d√©taill√©es par m√©tier pour les groupes significatifs
        metier_details = {}
        for file_type, df in dataframes.items():
            df_filtered = df[df["Top Conso"] == "O"].copy()
            if "M√©tier" in df_filtered.columns:
                print(significant_groups)
                if len(significant_groups) != 0:
                    # Filtrer pour les groupes significatifs seulement
                    df_significant = df_filtered[df_filtered["LCR_ECO_GROUPE_METIERS"].isin(significant_groups)]
                    
                    # Grouper par groupe m√©tier et m√©tier
                    grouped = df_significant.groupby(["LCR_ECO_GROUPE_METIERS", "M√©tier"])["LCR_ECO_IMPACT_LCR"].sum().reset_index()
                    grouped["LCR_ECO_IMPACT_LCR_Bn"] = (grouped["LCR_ECO_IMPACT_LCR"] / 1_000_000_000).round(3)
                    
                    # CONVERTIR EN DICTIONNAIRE S√âRIALISABLE
                    metier_details[file_type] = grouped.to_dict(orient='records')
        
        print (f"TESTSTSTSTSTSTST : {metier_detailed_analysis}")
        return {
            "title": "LCR Consumption Analysis by Business Group (Summary)",
            "consumption_table_html": consumption_html,
            "variations": variations,
            "analysis_text": analysis_text,
            "metier_detailed_analysis": metier_detailed_analysis, 
            "significant_groups": significant_groups,
            "metier_details": metier_details,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filter_applied": "Top Conso = 'O'",
                "grouping": ["LCR_ECO_GROUPE_METIERS"],  # Seulement groupe, pas m√©tier
                "measure": "LCR_ECO_IMPACT_LCR (Bn ‚Ç¨)",
                "view_type": "grouped_summary"
            }
        }
    
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation analyse Consumption group√©e: {e}")
        return {
            "title": "Consumption Analysis Grouped - Erreur",
            "error": str(e),
            "consumption_table_html": "<p class='text-danger'>Erreur lors de la g√©n√©ration de l'analyse Consumption group√©e</p>"
        }
    
def generate_consumption_grouped_table_html(consumption_grouped):
    """
    G√©n√®re le HTML du tableau Consumption GROUP√â (sans d√©tail m√©tiers)
    """
    if len(consumption_grouped) < 2:
        return "<div class='alert alert-warning'>Donn√©es insuffisantes pour l'analyse Consumption group√©e</div>"
    
    grouped_j = consumption_grouped.get("j")
    grouped_j1 = consumption_grouped.get("jMinus1")
    
    if grouped_j is None or grouped_j1 is None:
        return "<div class='alert alert-danger'>Erreur: donn√©es Consumption group√©es manquantes</div>"
    
    # Fusion de tous les groupes m√©tiers
    all_groups = set()
    all_groups.update(grouped_j["LCR_ECO_GROUPE_METIERS"].tolist())
    all_groups.update(grouped_j1["LCR_ECO_GROUPE_METIERS"].tolist())
    
    # Cr√©ation des dictionnaires de lookup
    lookup_j = grouped_j.set_index("LCR_ECO_GROUPE_METIERS")["LCR_ECO_IMPACT_LCR_Bn"].to_dict()
    lookup_j1 = grouped_j1.set_index("LCR_ECO_GROUPE_METIERS")["LCR_ECO_IMPACT_LCR_Bn"].to_dict()
    
    html = """
    <table class="table table-bordered consumption-table">
        <thead>
            <tr>
                <th rowspan="2" class="align-middle">LCR Groupe M√©tiers</th>
                <th colspan="2" class="text-center header-j-minus-1">D-1 (Yesterday)</th>
                <th colspan="2" class="text-center header-j">D (Today)</th>
                <th rowspan="2" class="text-center header-variation align-middle">Variation<br>(Bn ‚Ç¨)</th>
            </tr>
            <tr>
                <th class="text-center header-j-minus-1">Consumption (Bn ‚Ç¨)</th>
                <th class="text-center header-j-minus-1">Part (%)</th>
                <th class="text-center header-j">Consumption (Bn ‚Ç¨)</th>
                <th class="text-center header-j">Part (%)</th>
            </tr>
        </thead>
        <tbody>
    """
    
    # Calcul des totaux globaux pour les pourcentages
    total_j1 = sum(lookup_j1.values())
    total_j = sum(lookup_j.values())
    
    # G√©n√©ration des lignes - UNE LIGNE PAR GROUPE M√âTIER
    for groupe_metiers in sorted(all_groups):
        value_j1 = lookup_j1.get(groupe_metiers, 0)
        value_j = lookup_j.get(groupe_metiers, 0)
        variation = value_j - value_j1
        
        # Calcul des pourcentages
        pct_j1 = (value_j1 / total_j1 * 100) if total_j1 != 0 else 0
        pct_j = (value_j / total_j * 100) if total_j != 0 else 0
        
        html += '<tr>'
        html += f'<td class="fw-bold">{groupe_metiers}</td>'
        
        # Valeurs J-1
        html += f'<td class="text-end numeric-value">{value_j1:.3f}</td>'
        html += f'<td class="text-end numeric-value">{pct_j1:.1f}%</td>'
        
        # Valeurs J
        html += f'<td class="text-end numeric-value">{value_j:.3f}</td>'
        html += f'<td class="text-end numeric-value">{pct_j:.1f}%</td>'
        
        # Variation
        css_class = "variation-positive" if variation > 0 else "variation-negative" if variation < 0 else ""
        sign = "+" if variation > 0 else ""
        html += f'<td class="text-end numeric-value {css_class}">{sign}{variation:.3f}</td>'
        
        html += '</tr>'
    
    # Ligne de total g√©n√©ral
    total_variation = total_j - total_j1
    css_class = "variation-positive" if total_variation > 0 else "variation-negative" if total_variation < 0 else ""
    sign = "+" if total_variation > 0 else ""
    
    html += f'''
        <tr class="total-row">
            <td class="text-end fw-bold">TOTAL G√âN√âRAL:</td>
            <td class="text-end fw-bold">{total_j1:.3f}</td>
            <td class="text-end fw-bold">100.0%</td>
            <td class="text-end fw-bold">{total_j:.3f}</td>
            <td class="text-end fw-bold">100.0%</td>
            <td class="text-end fw-bold {css_class}">{sign}{total_variation:.3f}</td>
        </tr>
    '''
    
    html += """
        </tbody>
    </table>
    """
    
    return html

def calculate_consumption_grouped_variations(totals_by_group):
    """Calcule les variations de consumption group√©e entre J et J-1"""
    if "j" not in totals_by_group or "jMinus1" not in totals_by_group:
        return {}
    
    j_data = totals_by_group["j"]
    j1_data = totals_by_group["jMinus1"]
    
    # Variation globale
    global_variation = j_data["total_global"] - j1_data["total_global"]
    
    # Variations par groupe m√©tiers
    group_variations = {}
    all_groups = set(j_data["by_groupe_metiers"].keys()) | set(j1_data["by_groupe_metiers"].keys())
    
    for group in all_groups:
        j_value = j_data["by_groupe_metiers"].get(group, 0)
        j1_value = j1_data["by_groupe_metiers"].get(group, 0)
        group_variations[group] = {
            "j_minus_1": j1_value,
            "j": j_value,
            "variation": round(j_value - j1_value, 3)
        }
    
    return {
        "global": {
            "j_minus_1": j1_data["total_global"],
            "j": j_data["total_global"],
            "variation": round(global_variation, 3)
        },
        "by_groupe_metiers": group_variations
    }

def generate_consumption_grouped_analysis_text(variations, totals_by_group, dataframes=None):
    """G√©n√®re le texte d'analyse de la consumption group√©e avec mapping M√©tier -> Sous-M√©tier"""
    if not variations or "global" not in variations:
        return "Analyse Consumption group√©e non disponible - donn√©es insuffisantes.", []
    
    # Cr√©er le mapping M√©tier -> Sous-M√©tier depuis les donn√©es Excel IL SERT √Ä RIEN ICI ON UTILISE PAS M√âTIER MAIS √Ä R√âUTILISER POUR LES M√âTIER AU NIVEAU DE GRANULARIT√â SUIVANT.
    metier_to_sous_metier = {}
    if dataframes is not None and isinstance(dataframes, dict): #ATTENTION NE PAS METTRE if dataframes.
        # Utiliser le fichier J pour cr√©er le mapping (ou J-1 si J n'existe pas)
        df_for_mapping = dataframes.get("j")
        if df_for_mapping is None:
            df_for_mapping = dataframes.get("jMinus1")
        
        if df_for_mapping is not None:
            # V√©rifier que les colonnes existent
            if "M√©tier" in df_for_mapping.columns and "Sous-M√©tier" in df_for_mapping.columns:
                try:
                    # Cr√©er le mapping en supprimant les doublons
                    mapping_df = df_for_mapping[["M√©tier", "Sous-M√©tier"]].dropna().drop_duplicates()
                    metier_to_sous_metier = mapping_df.set_index("M√©tier")["Sous-M√©tier"].to_dict()
                    logger.info(f"Mapping M√©tier -> Sous-M√©tier cr√©√©: {len(metier_to_sous_metier)} entr√©es")
                except Exception as e:
                    logger.warning(f"Erreur cr√©ation mapping M√©tier->Sous-M√©tier: {e}")
            else:
                logger.warning("Colonnes 'M√©tier' ou 'Sous-M√©tier' non trouv√©es dans les donn√©es")
    
    global_data = variations["global"]
    date_str = datetime.now().strftime("March %d")
    
    # Analyse globale
    total_j = global_data["j"]
    variation = global_data["variation"]
    direction = "decrease" if variation < 0 else "increase"
    
    analysis = f"Summary view: on {date_str}, business groups have total consumption of {total_j:.2f} Bn, representing a {direction} of {abs(variation):.2f} Bn compared to yesterday."
    
    # Identification des principales variations par groupe (auto, sans param√®tre)
    significant_groups = []
    if "by_groupe_metiers" in variations and variations["by_groupe_metiers"]:
        def _tukey_upper_threshold(values):
            import statistics
            if len(values) < 4:
                return max(values) if values else 0.0
            q1 = statistics.quantiles(values, n=4)[0]
            q3 = statistics.quantiles(values, n=4)[2]
            iqr = q3 - q1
            return q3 + 1.5 * iqr

        def _knee_index(cum_shares):
            n = len(cum_shares)
            if n == 0:
                return None
            diffs = []
            for i, cs in enumerate(cum_shares, start=1):
                baseline = i / n
                diffs.append(cs - baseline)
            k = max(range(n), key=lambda i: diffs[i])
            return k if diffs[k] > 1e-9 else None

        by_grp = variations["by_groupe_metiers"]
        items = []
        for group, data in by_grp.items():
            gv = float(data.get("variation", 0.0))
            items.append((group, gv, abs(gv)))

        net_var = float(global_data.get("variation", 0.0))
        net_mag = abs(net_var)

        selected = []
        # CAS 1 : mouvement net significatif -> s√©lection des "drivers" align√©s (knee + outliers IQR)
        if net_mag >= 1e-9:
            sign = 1 if net_var >= 0 else -1
            aligned = [(g, v, av) for (g, v, av) in items if (v > 0 and sign > 0) or (v < 0 and sign < 0)]
            aligned.sort(key=lambda x: x[2], reverse=True)

            cum = 0.0
            cum_shares = []
            for _, _, av in aligned:
                cum += av
                cum_shares.append(min(cum / net_mag, 1.0))

            knee = _knee_index(cum_shares)
            if knee is not None:
                selected = aligned[:knee+1]

            aligned_abs = [av for (_, _, av) in aligned]
            upper = _tukey_upper_threshold(aligned_abs)
            for tup in aligned:
                if tup not in selected and tup[2] >= upper:
                    selected.append(tup)

            if not selected and aligned:
                selected = [aligned[0]]  # au moins le plus gros driver

        # CAS 2 : net ~ 0 -> sortir les vrais movers (IQR) des deux c√¥t√©s
        else:
            abs_vars = [av for (_, _, av) in items]
            upper = _tukey_upper_threshold(abs_vars)
            movers = [(g, v, av) for (g, v, av) in items if av >= upper]
            movers.sort(key=lambda x: x[2], reverse=True)
            if not movers and items:
                movers = [max(items, key=lambda x: x[2])]
            selected = movers

        # Mise en forme avec mapping M√©tier -> Sous-M√©tier
        significant_variations = []
        for g, v, av in selected:
            sign_sym = "-" if v < 0 else "+"
            
            # Utiliser le mapping pour obtenir le nom complet
            display_name = metier_to_sous_metier.get(g, g)  # Si pas de mapping trouv√©, utiliser g
            
            significant_variations.append(f"{display_name} ({sign_sym}{abs(v):.2f} Bn)")
            significant_groups.append(g)  # Garder l'abr√©viation pour les traitements ult√©rieurs

        if significant_variations:
            if variation < 0:
                analysis += f" Main contributors to this decrease: {', '.join(significant_variations)}."
            else:
                analysis += f" Main drivers of this increase: {', '.join(significant_variations)}."
    
    return analysis, significant_groups

def generate_metier_detailed_analysis(significant_groups, dataframes=None):
    """
    G√©n√®re une analyse textuelle d√©taill√©e des m√©tiers avec les plus grosses variations
    en recr√©ant les donn√©es m√©tier depuis les DataFrames
    """
    if not significant_groups or not dataframes:
        return ""
    
    logger.info(f"G√©n√©ration analyse d√©taill√©e pour groupes: {significant_groups}")
    
    # Cr√©er le mapping M√©tier -> Sous-M√©tier
    metier_to_sous_metier = {}
    if dataframes is not None and isinstance(dataframes, dict):
        df_for_mapping = dataframes.get("j")
        if df_for_mapping is None:
            df_for_mapping = dataframes.get("jMinus1")
        
        if df_for_mapping is not None:
            has_metier = "M√©tier" in df_for_mapping.columns
            has_sous_metier = "Sous-M√©tier" in df_for_mapping.columns
            
            if has_metier and has_sous_metier:
                try:
                    mapping_df = df_for_mapping[["M√©tier", "Sous-M√©tier"]].dropna().drop_duplicates()
                    metier_to_sous_metier = mapping_df.set_index("M√©tier")["Sous-M√©tier"].to_dict()
                    logger.info(f"Mapping M√©tier -> Sous-M√©tier cr√©√© pour analyse d√©taill√©e: {len(metier_to_sous_metier)} entr√©es")
                except Exception as e:
                    logger.warning(f"Erreur cr√©ation mapping pour analyse d√©taill√©e: {e}")
    
    # Recr√©er les donn√©es m√©tier depuis les DataFrames
    metier_data = {}
    
    try:
        for file_type, df in dataframes.items():
            df_filtered = df[df["Top Conso"] == "O"].copy()
            
            # V√©rifier si la colonne M√©tier existe
            if "M√©tier" not in df_filtered.columns:
                logger.warning(f"Colonne 'M√©tier' non trouv√©e dans {file_type}, analyse d√©taill√©e impossible")
                continue
            
            if len(significant_groups) > 0:
                # Filtrer pour les groupes significatifs seulement
                df_significant = df_filtered[df_filtered["LCR_ECO_GROUPE_METIERS"].isin(significant_groups)]
                
                if df_significant.empty:  # Utiliser .empty au lieu de len() == 0
                    logger.warning(f"Aucune donn√©e pour les groupes significatifs dans {file_type}")
                    continue
                
                # Grouper par groupe m√©tier et m√©tier
                grouped = df_significant.groupby(["LCR_ECO_GROUPE_METIERS", "M√©tier"])["LCR_ECO_IMPACT_LCR"].sum().reset_index()
                grouped["LCR_ECO_IMPACT_LCR_Bn"] = (grouped["LCR_ECO_IMPACT_LCR"] / 1_000_000_000).round(3)
                
                metier_data[file_type] = grouped
                logger.info(f"Donn√©es m√©tier cr√©√©es pour {file_type}: {len(grouped)} lignes")
    
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation des donn√©es m√©tier: {e}")
        return ""

    # V√©rifier que nous avons les donn√©es J et J-1
    if "j" not in metier_data or "jMinus1" not in metier_data:
        logger.warning("Donn√©es J ou J-1 manquantes pour l'analyse d√©taill√©e")
        return ""
    
    data_j = metier_data["j"]
    data_j1 = metier_data["jMinus1"]
    
    # V√©rifier que les DataFrames ne sont pas vides
    if data_j.empty or data_j1.empty:
        logger.warning("DataFrames J ou J-1 vides pour l'analyse d√©taill√©e")
        return ""
    
    # Cr√©er des dictionnaires de lookup par (groupe, m√©tier)
    lookup_j = {}
    lookup_j1 = {}
    
    try:
        for _, row in data_j.iterrows():
            key = (row["LCR_ECO_GROUPE_METIERS"], row["M√©tier"])
            lookup_j[key] = row["LCR_ECO_IMPACT_LCR_Bn"]
        
        for _, row in data_j1.iterrows():
            key = (row["LCR_ECO_GROUPE_METIERS"], row["M√©tier"])
            lookup_j1[key] = row["LCR_ECO_IMPACT_LCR_Bn"]
    
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation des dictionnaires lookup: {e}")
        return ""
    
# Calculer les variations par m√©tier
    all_keys = set(lookup_j.keys()) | set(lookup_j1.keys())
    metier_variations = []
    
    for key in all_keys:
        groupe, metier = key
        if groupe in significant_groups:  # Seulement les groupes significatifs
            value_j = lookup_j.get(key, 0)
            value_j1 = lookup_j1.get(key, 0)
            variation = value_j - value_j1
            
            # Utiliser le mapping pour obtenir le nom complet
            display_name = metier_to_sous_metier.get(metier, metier)
            
            metier_variations.append({
                "groupe": groupe,
                "metier": metier,
                "display_name": display_name,
                "variation": variation,
                "abs_variation": abs(variation),
                "value_j": value_j,
                "value_j1": value_j1
            })
    
    # NOUVEAU: Grouper par groupe m√©tier et prendre le top 3 de chaque groupe
    variations_by_group = {}
    for variation in metier_variations:
        groupe = variation["groupe"]
        if groupe not in variations_by_group:
            variations_by_group[groupe] = []
        variations_by_group[groupe].append(variation)
    
    # Trier chaque groupe par variation absolue d√©croissante et prendre le top 3
    top_variations_by_group = {}
    for groupe, variations in variations_by_group.items():
        # Trier par variation absolue d√©croissante
        sorted_variations = sorted(variations, key=lambda x: x["abs_variation"], reverse=True)
        # Prendre les 3 premi√®res (ou moins si moins de 3 m√©tiers)
        top_variations_by_group[groupe] = sorted_variations[:3]
    
    # G√©n√©rer le texte d'analyse
    date_str = datetime.now().strftime("March %d")
    group_sentences = []
    
    # Traiter chaque groupe s√©par√©ment pour cr√©er des phrases distinctes
    for groupe in significant_groups:
        if groupe in top_variations_by_group:
            group_variations = top_variations_by_group[groupe]
            group_parts = []
            
            for item in group_variations:
                variation = item["variation"]
                abs_variation = item["abs_variation"]
                display_name = item["display_name"]
                
                # Ignorer les variations tr√®s faibles
                if abs_variation < 0.01:  # Moins de 10M‚Ç¨
                    continue
                
                direction = "increased" if variation > 0 else "decreased"
                group_parts.append(f"{display_name} {direction} by {abs_variation:.2f} Bn")
            
            # Cr√©er une phrase compl√®te pour ce groupe avec le nom du groupe en gras
            if group_parts:
                group_sentence = f"In <strong>{groupe}</strong>, {', '.join(group_parts)}"
                group_sentences.append(group_sentence)
    
    if group_sentences:
        # Joindre les phrases avec ". " pour s√©parer chaque groupe
        full_text = ". ".join(group_sentences)
        return f"At the detailed level: {full_text}."
    
    return ""


if __name__ == "__main__":
    print("üöÄ Steering ALM Metrics - Version Templates")
    print("üìä Interface: http://localhost:8000")
    print("üìÅ Templates: templates/index.html")
    print("üé® Styles: static/js/main.js")
    print("‚èπÔ∏è  Ctrl+C pour arr√™ter")
    
    uvicorn.run(
        app,
        host="localhost",
        port=8000,
        reload=False,
        log_level="info",
        timeout_keep_alive=300,  # 5 minutes
        limit_max_requests=1000
    )