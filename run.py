#!/usr/bin/env python3
#run.py
"""
Steering ALM Metrics - Version avec Templates
============================================

Application FastAPI utilisant un syst√®me de templates Jinja2
pour s√©parer la logique m√©tier de la pr√©sentation.
"""

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request, Cookie
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import pandas as pd
import uvicorn
import logging
from pathlib import Path
from datetime import datetime
import io
import chardet
from typing import Optional
import psutil
import os
from user import authenticate_user, log_activity, get_logs, USERS_DB
import secrets

from llm_connector import LLMConnector
from report_generator import ReportGenerator

# Initialiser le connecteur LLM
llm_connector = LLMConnector()

# Formats support√©s
SUPPORTED_EXTENSIONS = ['.xlsx', '.xls', '.xlsm', '.xlsb', '.csv', '.tsv', '.txt']

# Variables globales pour la session chatbot
chatbot_session = {
    "messages": [],
    "context_data": {},
    "uploaded_documents": []
}
# Variables globales pour la session (en production: utiliser une base de donn√©es)
file_session = {"files": {}}
# Session utilisateur global (en production: vraies sessions SSH si possible)
active_sessions = {}

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cr√©ation des dossiers requis
required_dirs = ["data", "templates", "static", "static/js", "static/css", "static/images"]
for directory in required_dirs:
    Path(directory).mkdir(exist_ok=True)

required_cols = ["Top Conso", "LCR_Cat√©gorie", "LCR_Template Section 1", "Libell√© Client", 
                "LCR_Assiette Pond√©r√©e", "LCR_ECO_GROUPE_METIERS", "Sous-M√©tier", "Produit", 
                "LCR_ECO_IMPACT_LCR", "SI Remettant", "Commentaire", "Date d'arr√™t√©"]
                
# Cr√©ation de l'application FastAPI
app = FastAPI(title="Steering ALM Metrics", version="2.0.0")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration des fichiers statiques et templates
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")


# =========================== FONCTIONS UTILITAIRES ===========================


def generate_session_token():
    return secrets.token_urlsafe(32)

def get_current_user_from_session(session_token: Optional[str] = Cookie(None)):
    """R√©cup√®re l'utilisateur depuis le token de session"""
    if not session_token or session_token not in active_sessions:
        return None
    return active_sessions[session_token]

def convert_file_content_to_dataframe(file_content: bytes, filename: str):
    """
    Convertit le contenu d'un fichier directement en DataFrame
    Sans √©criture sur disque
    """
    try:
        extension = Path(filename).suffix.lower()
        
        # Excel - lecture directe depuis bytes
        if extension in ['.xlsx', '.xls', '.xlsm', '.xlsb']:
            df = pd.read_excel(io.BytesIO(file_content), engine='openpyxl')
            df.columns = df.columns.astype(str).str.strip()
            return df, {'format': extension, 'method': 'direct_excel'}
        
        # CSV/TSV/TXT - traitement en m√©moire
        elif extension in ['.csv', '.tsv', '.txt']:
            # D√©tecter l'encodage
            encoding = chardet.detect(file_content[:10000])['encoding'] or 'utf-8'
            
            # D√©coder en string
            text_content = file_content.decode(encoding)
            
            # D√©tecter le d√©limiteur
            if extension == '.tsv':
                delimiter = '\t'
            else:
                first_line = text_content.split('\n')[0]
                if ';' in first_line:
                    delimiter = ';'
                elif '\t' in first_line:
                    delimiter = '\t'
                else:
                    delimiter = ','
            
            # Lire directement depuis StringIO (en m√©moire)
            df = pd.read_csv(
                io.StringIO(text_content),
                delimiter=delimiter,
                low_memory=False,
                dtype=str,
                na_filter=False
            )
            
            # Nettoyer les colonnes
            df.columns = df.columns.astype(str).str.strip()
            
            # Convertir les colonnes num√©riques
            numeric_columns = ['Nominal Value', 'LCR_ECO_IMPACT_LCR']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col].str.replace(',', '.'), errors='coerce')
            
            return df, {
                'format': extension, 
                'encoding': encoding, 
                'delimiter': delimiter,
                'method': 'memory_csv'
            }
        
        else:
            raise ValueError(f"Format non support√©: {extension}")
            
    except Exception as e:
        raise ValueError(f"Erreur lecture fichier: {str(e)}")

def cleanup_session_memory():
    """Nettoie la m√©moire des DataFrames de session"""
    try:
        if "files" in file_session:
            for file_type in list(file_session["files"].keys()):
                if file_type in file_session["files"] and "dataframe" in file_session["files"][file_type]:
                    del file_session["files"][file_type]["dataframe"]
                    logger.info(f"DataFrame {file_type} supprim√© de la session")
        
        # Nettoyage complet
        file_session["files"].clear()
        
        import gc
        gc.collect()
        
        # V√©rifier la m√©moire apr√®s nettoyage
        process = psutil.Process(os.getpid())
        memory_after = process.memory_info().rss / 1024 / 1024
        logger.info(f"M√©moire apr√®s nettoyage complet: {memory_after:.1f} MB")
        
    except Exception as e:
        logger.warning(f"Erreur nettoyage m√©moire: {e}")


# ========================== ENDPOINTS EXPORT ===========================  


@app.get("/health")
async def health_check():
    """Endpoint de v√©rification de l'√©tat de l'application"""
    return {
        "status": "healthy",
        "service": "steering-alm-metrics",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "active_files": len(file_session.get("files", {})),
        "templates_available": Path("templates/index.html").exists(),
        "static_available": Path("static/js/main.js").exists()
    }


# =========================== ENDPOINTS AUTHENTIFICATION ===========================


@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """Page d'accueil - redirige vers login si non connect√©"""
    session_token = request.cookies.get("session_token")
    current_user = get_current_user_from_session(session_token)
    
    if not current_user:
        # Utilisateur non connect√© -> page de login
        return templates.TemplateResponse("login.html", {
            "request": request,
            "title": "Login - Steering ALM Metrics"
        })
    
    # Utilisateur connect√© -> page principale
    log_activity(current_user["username"], "ACCESS", "Accessed main page")
    
    return templates.TemplateResponse("index.html", {
        "request": request,
        "title": "Steering ALM Metrics",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "user": current_user
    })

@app.post("/api/login")
async def login(request: Request):
    """Authentification utilisateur"""
    try:
        data = await request.json()
        username = data.get("username")
        password = data.get("password")
        
        user = authenticate_user(username, password)
        if not user:
            raise HTTPException(status_code=401, detail="Invalid credentials")
        
        # Cr√©er une session
        session_token = generate_session_token()
        active_sessions[session_token] = user
        
        log_activity(username, "LOGIN", "Successful login")
        
        response = JSONResponse({
            "success": True,
            "message": f"Welcome {user['full_name']}!",
            "redirect": "/"
        })
        
        # D√©finir le cookie de session
        response.set_cookie(
            key="session_token",
            value=session_token,
            httponly=True,
            max_age=24*60*60,  # 24 heures
            samesite="lax"
        )
        
        return response
        
    except Exception as e:
        return JSONResponse(
            {"success": False, "message": str(e)}, 
            status_code=401
        )

@app.post("/api/logout")
async def logout(request: Request):
    """D√©connexion utilisateur"""
    session_token = request.cookies.get("session_token")
    
    if session_token and session_token in active_sessions:
        user = active_sessions[session_token]
        log_activity(user["username"], "LOGOUT", "User logged out")
        del active_sessions[session_token]
    
    response = JSONResponse({"success": True, "redirect": "/"})
    response.delete_cookie("session_token")
    return response


# =========================== ENDPOINTS FICHIERS ===========================


@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...), 
                     file_type: str = Form(...),
                     session_token: Optional[str] = Cookie(None)):
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    log_activity(current_user["username"], "FILE_UPLOAD", f"Uploaded {file.filename} as {file_type}")
    try:
        # MONITORING INITIAL
        process = psutil.Process(os.getpid())
        memory_start = process.memory_info().rss / 1024 / 1024
        logger.info(f"M√©moire avant upload {file_type}: {memory_start:.1f} MB")

        logger.info(f"Upload re√ßu: {file.filename}, type: {file_type}")
        
        # Validation du fichier
        if not file.filename:
            raise HTTPException(status_code=400, detail="Nom de fichier manquant")
        
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in SUPPORTED_EXTENSIONS:
            raise HTTPException(
                status_code=400, 
                detail=f"Format non support√©: {file_extension}"
            )

        # Lire le contenu en m√©moire
        contents = await file.read()
        file_size = len(contents)
        
        # TRAITEMENT DIRECT EN M√âMOIRE
        try:
            df, file_info = convert_file_content_to_dataframe(contents, file.filename)
            logger.info(f"Fichier trait√© en m√©moire: {file_info}")
            
        except Exception as e:
            del contents
            raise HTTPException(status_code=422, detail=f"Erreur lecture: {str(e)}")
        
        # Sauvegarder les infos AVANT filtrage
        original_rows = len(df)
        original_columns = len(df.columns)

        # Filtrer seulement les lignes n√©cessaires
        df_filtered = df[df["Top Conso"] == "O"].copy() if "Top Conso" in df.columns else df.copy()

        # OPTIMISATION M√âMOIRE - Ne garder que les colonnes utiles
        available_cols = [col for col in required_cols if col in df_filtered.columns]
        df_minimal = df_filtered[available_cols].copy()

        # Optimiser les types de donn√©es
        for col in df_minimal.select_dtypes(include=['float64']):
            df_minimal[col] = pd.to_numeric(df_minimal[col], downcast='float')

        # LIB√âRATION M√âMOIRE IMM√âDIATE
        del df_filtered
        del df
        del contents
        import gc
        gc.collect()

        # Diagnostic m√©moire
        logger.info(f"DataFrame optimis√©: {df_minimal.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
        logger.info(f"Shape finale: {df_minimal.shape}")

        # Stocker le DataFrame optimis√©
        file_session["files"][file_type] = {
            "dataframe": df_minimal,  # DataFrame optimis√©
            "original_name": file.filename,
            "file_format": file_info['format'],
            "encoding": file_info.get('encoding'),
            "delimiter": file_info.get('delimiter'),
            "rows": len(df_minimal),
            "columns": len(df_minimal.columns),
            "upload_time": datetime.now().isoformat(),
        }

        # MONITORING FINAL
        memory_end = process.memory_info().rss / 1024 / 1024
        logger.info(f"M√©moire apr√®s upload {file_type}: {memory_end:.1f} MB (diff: +{memory_end-memory_start:.1f} MB)")
        
        return {
            "success": True,
            "message": f"Fichier {file_type} trait√© en m√©moire ({file_info['format']})",
            "filename": file.filename,
            "format": file_info['format'],
            "encoding": file_info.get('encoding'),
            "delimiter": file_info.get('delimiter'),
            "rows": original_rows,
            "columns": original_columns,
            "file_size": file_size,
            "processing": "in_memory_optimized"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur upload: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")
    
@app.post("/api/cleanup-memory")
async def cleanup_memory_endpoint(session_token: Optional[str] = Cookie(None)):
    """Endpoint pour nettoyer la m√©moire manuellement"""
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    try:
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024 / 1024
        
        cleanup_session_memory()
        
        memory_after = process.memory_info().rss / 1024 / 1024
        memory_freed = memory_before - memory_after
        
        log_activity(current_user["username"], "MEMORY_CLEANUP", f"Freed {memory_freed:.1f} MB")
        
        return {
            "success": True,
            "message": f"Memory cleaned: {memory_freed:.1f} MB freed",
            "memory_before": memory_before,
            "memory_after": memory_after
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cleanup error: {str(e)}")


# ========================== ENDPOINTS ANALYSE ===========================


@app.post("/api/analyze")
async def analyze_files(session_token: Optional[str] = Cookie(None)):
    # V√©rifier l'authentification
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    # Logger l'activit√©
    log_activity(current_user["username"], "ANALYSIS", "Started LCR analysis")
    try:
        logger.info("D√©but de l'analyse depuis DataFrames en m√©moire")
        
        # V√©rification de la pr√©sence des deux fichiers
        if len(file_session.get("files", {})) < 2:
            raise HTTPException(status_code=400, detail="Les deux fichiers sont requis")
        
        if "j" not in file_session["files"] or "jMinus1" not in file_session["files"]:
            raise HTTPException(status_code=400, detail="Fichiers manquants")
        
        # R√©cup√©rer les DataFrames directement depuis la session
        dataframes = {}
        for file_type, file_info in file_session["files"].items():
            df = file_info["dataframe"]  # DataFrame d√©j√† en m√©moire
            dataframes[file_type] = df
            logger.info(f"{file_type}: {len(df)} lignes (depuis m√©moire)")

        # Nouvelles analyses
        buffer_results = create_buffer_table(dataframes)
        consumption_results = create_consumption_table(dataframes)
        resources_results = create_resources_table(dataframes)
        cappage_results = create_cappage_table(dataframes)
        buffer_nco_results = create_buffer_nco_table(dataframes)
        
        logger.info("Analyses termin√©es (nouveaux tableaux)")

        # SAUVEGARDER LE CONTEXTE CHATBOT
        chatbot_session["context_data"] = {
            "buffer": buffer_results,
            "consumption": consumption_results,
            "resources": resources_results,
            "cappage": cappage_results,
            "buffer_nco": buffer_nco_results,
            "analysis_timestamp": datetime.now().isoformat(),
            "raw_dataframes_info": {
                file_type: {
                    "shape": [len(df), len(df.columns)],
                    "columns": df.columns.tolist(),
                    "sample_data": df.head(3).to_dict('records') if len(df) > 0 else [],
                    "file_info": file_session["files"][file_type]
                }
                for file_type, df in dataframes.items()
            }
        }

        return {
            "success": True,
            "message": "Analyses termin√©es avec nouveaux tableaux",
            "timestamp": datetime.now().isoformat(),
            "context_ready": True,  
            "results": {
                "buffer": buffer_results,
                "consumption": consumption_results,
                "resources": resources_results,
                "cappage": cappage_results,
                "buffer_nco": buffer_nco_results
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur analyse: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur d'analyse: {str(e)}")
    
@app.get("/api/context-status")
async def get_context_status():
    """V√©rifie si le contexte du chatbot est pr√™t"""
    has_context = bool(chatbot_session.get("context_data"))
    context_keys = list(chatbot_session.get("context_data", {}).keys()) if has_context else []
    
    return {
        "context_ready": has_context,
        "context_keys": context_keys,
        "timestamp": datetime.now().isoformat(),
        "analysis_timestamp": chatbot_session.get("context_data", {}).get("analysis_timestamp")
    }


# ========================== ENDPOINTS CHATBOT ===========================


@app.post("/api/chat")
async def chat_with_ai(request: Request, session_token: Optional[str] = Cookie(None)):
    """
    Endpoint pour le chatbot IA
    """
    # V√©rifier l'authentification
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    try:
        data = await request.json()
        user_message = data.get("message", "")
        
        if not user_message.strip():
            raise HTTPException(status_code=400, detail="Message vide")
        
        # NOUVEAU LOG : Message envoy√© √† l'IA
        log_activity(current_user["username"], "CHAT_MESSAGE", f"Sent message to AI: {user_message[:100]}{'...' if len(user_message) > 100 else ''}")
        
        # Pr√©parer le contexte complet avec historique
        context_prompt = prepare_conversation_context()
        
        # Obtenir la r√©ponse de l'IA
        ai_response = llm_connector.get_llm_response(
            user_prompt=user_message,
            context_prompt=context_prompt,
            modelID="gpt-4o-mini-2024-07-18",
            temperature=0.1
        )
        
        # Sauvegarder la conversation
        chatbot_session["messages"].append({
            "type": "user",
            "message": user_message,
            "timestamp": datetime.now().isoformat()
        })
        
        chatbot_session["messages"].append({
            "type": "assistant",
            "message": ai_response,
            "timestamp": datetime.now().isoformat()
        })
        
        return {
            "success": True,
            "response": ai_response,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Erreur chatbot: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur chatbot: {str(e)}")

@app.post("/api/upload-document")
async def upload_document(file: UploadFile = File(...), session_token: Optional[str] = Cookie(None)):
    """
    Upload de documents pour le contexte du chatbot
    """
    # V√©rifier l'authentification
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="Nom de fichier manquant")
        
        # NOUVEAU LOG : Document upload√© pour contexte
        log_activity(current_user["username"], "DOCUMENT_UPLOAD", f"Uploaded context document: {file.filename}")
        
        contents = await file.read()
        
        # Traiter selon le type de fichier
        if file.filename.endswith('.txt'):
            text_content = contents.decode('utf-8')
        elif file.filename.endswith('.pdf'):
            # Vous devrez installer PyPDF2 : pip install PyPDF2
            import PyPDF2
            from io import BytesIO
            pdf_reader = PyPDF2.PdfReader(BytesIO(contents))
            text_content = ""
            for page in pdf_reader.pages:
                text_content += page.extract_text()
        else:
            text_content = contents.decode('utf-8', errors='ignore')
        
        # Sauvegarder le document
        doc_data = {
            "filename": file.filename,
            "content": text_content,
            "upload_time": datetime.now().isoformat(),
            "size": len(contents)
        }
        
        chatbot_session["uploaded_documents"].append(doc_data)
        
        return {
            "success": True,
            "message": f"Document {file.filename} ajout√© au contexte",
            "filename": file.filename,
            "size": len(contents)
        }
        
    except Exception as e:
        logger.error(f"Erreur upload document: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

@app.get("/api/uploaded-documents")
async def get_uploaded_documents():
    """
    R√©cup√®re la liste d√©taill√©e des documents upload√©s
    """
    documents = []
    for doc in chatbot_session.get("uploaded_documents", []):
        documents.append({
            "filename": doc["filename"],
            "upload_time": doc["upload_time"],
            "size": doc["size"]
        })
    
    return {
        "success": True,
        "documents": documents,
        "count": len(documents)
    }

@app.get("/api/chat-history")
async def get_chat_history():
    """
    R√©cup√®re l'historique des messages du chatbot
    """
    return {
        "success": True,
        "messages": chatbot_session["messages"],
        "documents_count": len(chatbot_session["uploaded_documents"])
    }

@app.delete("/api/chat-clear")
async def clear_chat():
    """
    Vide l'historique du chatbot
    """
    chatbot_session["messages"].clear()
    chatbot_session["uploaded_documents"].clear()
    return {"success": True, "message": "Historique effac√©"}


# ========================== ENDPOINTS ADMIN ===========================


@app.get("/api/logs")
async def get_activity_logs(session_token: Optional[str] = Cookie(None), limit: int = 100):
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
        
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    logs = get_logs(limit)
    return {
        "success": True,
        "logs": logs,
        "total": len(logs)
    }

@app.get("/api/logs-stats")
async def get_logs_statistics(session_token: Optional[str] = Cookie(None)):
    """Statistiques sur les logs (admins seulement)"""
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
        
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    from user import get_logs_stats
    stats = get_logs_stats()
    
    return {
        "success": True,
        "stats": stats
    }

@app.get("/api/users")
async def get_users_list(session_token: Optional[str] = Cookie(None)):
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
        
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    users = [
        {
            "username": user["username"],
            "full_name": user["full_name"], 
            "role": user["role"],
            "created_at": user["created_at"]
        }
        for user in USERS_DB.values()
    ]
    
    return {
        "success": True,
        "users": users
    }


# ========================== ENDPOINTS EXPORT ===========================  


@app.post("/api/export-pdf")
async def export_pdf(session_token: Optional[str] = Cookie(None)):
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    # V√©rifier qu'une analyse existe
    if not chatbot_session.get("context_data"):
        raise HTTPException(status_code=400, detail="No analysis available")
    
    # Retourner juste l'URL de visualisation
    return JSONResponse({
        "success": True,
        "report_url": "/view-report"
    })  

@app.get("/view-report")
async def view_current_report(session_token: Optional[str] = Cookie(None)):
    """Affiche le dernier rapport g√©n√©r√©"""
    current_user = get_current_user_from_session(session_token)
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    try:
        # V√©rifier qu'une analyse existe
        if not chatbot_session.get("context_data"):
            raise HTTPException(status_code=400, detail="No analysis available")
        
        # G√©n√©rer le rapport √† la vol√©e
        last_ai_response = None
        if chatbot_session.get("messages"):
            ai_messages = [msg for msg in chatbot_session["messages"] if msg["type"] == "assistant"]
            if ai_messages:
                last_ai_response = ai_messages[-1]["message"]
        
        generator = ReportGenerator(
            analysis_results={
                "balance_sheet": chatbot_session["context_data"].get("balance_sheet"),
                "consumption": chatbot_session["context_data"].get("consumption")
            },
            last_ai_response=last_ai_response
        )
        
        # Capturer graphiques et g√©n√©rer HTML
        generator.chart_images = generator.capture_charts_with_html2image()
        html_content = generator.generate_print_html()
        
        # Retourner directement le HTML
        return HTMLResponse(content=html_content)
        
    except Exception as e:
        return HTMLResponse(content=f"<h1>Erreur g√©n√©ration rapport</h1><p>{str(e)}</p>")
    


# ========================== FONCTIONS BUFFER TABLE ===========================


def create_buffer_table(dataframes):
    """
    Cr√©e le tableau BUFFER avec filtres sp√©cifiques
    """
    try:
        logger.info("üìä Cr√©ation du tableau BUFFER")
        
        buffer_results = {}
        
        for file_type, df in dataframes.items():
            logger.info(f"üìÑ Traitement BUFFER pour {file_type}")
            
            # V√©rification des colonnes requises
            buffer_cols = ["Top Conso", "LCR_Cat√©gorie", "LCR_Template Section 1", 
                          "Libell√© Client", "LCR_Assiette Pond√©r√©e"]
            missing_cols = [col for col in buffer_cols if col not in df.columns]
            
            if missing_cols:
                logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour BUFFER {file_type}: {missing_cols}")
                continue
            
            # Filtrage des donn√©es
            df_filtered = df[df["Top Conso"] == "O"].copy()
            df_filtered = df_filtered[df_filtered["LCR_Cat√©gorie"] == "1- Buffer"].copy()
            
            logger.info(f"üìã Apr√®s filtrage BUFFER: {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e BUFFER pour {file_type}")
                continue
            
            # Pr√©paration des donn√©es
            df_filtered["LCR_Assiette Pond√©r√©e"] = pd.to_numeric(
                df_filtered["LCR_Assiette Pond√©r√©e"], errors='coerce'
            ).fillna(0)
            
            # Nettoyage des champs texte
            df_filtered["LCR_Template Section 1"] = df_filtered["LCR_Template Section 1"].astype(str).str.strip()
            df_filtered["Libell√© Client"] = df_filtered["Libell√© Client"].astype(str).str.strip()
            
            # Groupement
            grouped_data = []
            
            # Grouper par LCR_Template Section 1
            for section in df_filtered["LCR_Template Section 1"].unique():
                section_data = df_filtered[df_filtered["LCR_Template Section 1"] == section]
                
                if section == "1.1- Cash":
                    # Pour 1.1- Cash, montrer le d√©tail par Libell√© Client
                    for client in section_data["Libell√© Client"].unique():
                        client_data = section_data[section_data["Libell√© Client"] == client]
                        total = float(client_data["LCR_Assiette Pond√©r√©e"].sum())
                        grouped_data.append({
                            "section": section,
                            "client": client,
                            "total": total / 1_000_000_000,  # Conversion en milliards
                            "is_detail": True
                        })
                else:
                    # Pour les autres sections, montrer seulement le total
                    total = float(section_data["LCR_Assiette Pond√©r√©e"].sum())
                    grouped_data.append({
                        "section": section,
                        "client": "TOTAL",
                        "total": total / 1_000_000_000,  # Conversion en milliards
                        "is_detail": False
                    })
            
            buffer_results[file_type] = grouped_data
            logger.info(f"‚úÖ BUFFER {file_type}: {len(grouped_data)} entr√©es")
        
        return {
            "title": "BUFFER",
            "data": buffer_results,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filters_applied": {
                    "top_conso": "O",
                    "lcr_categorie": "1- Buffer"
                }
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation tableau BUFFER: {e}")
        return {
            "title": "BUFFER - Erreur",
            "error": str(e)
        }


# ========================== FONCTIONS CONSUMPTION TABLE ===========================


def create_consumption_table(dataframes):
    """
    Cr√©e le tableau CONSUMPTION avec filtres sp√©cifiques
    """
    try:
        logger.info("üìä Cr√©ation du tableau CONSUMPTION")
        
        consumption_results = {}
        
        for file_type, df in dataframes.items():
            logger.info(f"üìÑ Traitement CONSUMPTION pour {file_type}")
            
            # V√©rification des colonnes requises
            consumption_cols = ["Top Conso", "LCR_ECO_GROUPE_METIERS", "Sous-M√©tier", 
                              "Produit", "LCR_ECO_IMPACT_LCR"]
            missing_cols = [col for col in consumption_cols if col not in df.columns]
            
            if missing_cols:
                logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour CONSUMPTION {file_type}: {missing_cols}")
                continue
            
            # Filtrage des donn√©es
            df_filtered = df[df["Top Conso"] == "O"].copy()
            
            # Filtres sp√©cifiques CONSUMPTION
            allowed_groupes = ["A&WM & Insurance", "CIB Financing", "CIB Markets", "GLOBAL TRADE", "Other Consumption"]
            df_filtered = df_filtered[df_filtered["LCR_ECO_GROUPE_METIERS"].isin(allowed_groupes)].copy()
            
            excluded_sous_metier = ["GT TREASURY SOLUTIONS", "GT GROUP SERVICES"]
            df_filtered = df_filtered[~df_filtered["Sous-M√©tier"].isin(excluded_sous_metier)].copy()
            
            excluded_produit = ["SIGHT DEPOSIT MIRROR", "SIGHT FINANCING MIRROR"]
            df_filtered = df_filtered[~df_filtered["Produit"].isin(excluded_produit)].copy()
            
            logger.info(f"üìã Apr√®s filtrage CONSUMPTION: {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e CONSUMPTION pour {file_type}")
                continue
            
            # Pr√©paration des donn√©es
            df_filtered["LCR_ECO_IMPACT_LCR"] = pd.to_numeric(
                df_filtered["LCR_ECO_IMPACT_LCR"], errors='coerce'
            ).fillna(0)

            # Groupement par LCR_ECO_GROUPE_METIERS
            grouped = df_filtered.groupby("LCR_ECO_GROUPE_METIERS")["LCR_ECO_IMPACT_LCR"].sum().reset_index()
            grouped["LCR_ECO_IMPACT_LCR_Bn"] = (grouped["LCR_ECO_IMPACT_LCR"] / 1_000_000_000).round(3)

            # Convertir en dictionnaire avec types Python natifs
            consumption_results[file_type] = [
                {
                    "LCR_ECO_GROUPE_METIERS": str(row["LCR_ECO_GROUPE_METIERS"]),
                    "LCR_ECO_IMPACT_LCR": float(row["LCR_ECO_IMPACT_LCR"]),
                    "LCR_ECO_IMPACT_LCR_Bn": float(row["LCR_ECO_IMPACT_LCR_Bn"])
                }
                for _, row in grouped.iterrows()
            ]
            logger.info(f"‚úÖ CONSUMPTION {file_type}: {len(grouped)} groupes")
        
        return {
            "title": "CONSUMPTION",
            "data": consumption_results,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filters_applied": {
                    "top_conso": "O",
                    "groupe_metiers": allowed_groupes,
                    "excluded_sous_metier": excluded_sous_metier,
                    "excluded_produit": excluded_produit
                }
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation tableau CONSUMPTION: {e}")
        return {
            "title": "CONSUMPTION - Erreur",
            "error": str(e)
        }


# ========================== FONCTIONS RESOURCES TABLE ===========================


def create_resources_table(dataframes):
    """
    Cr√©e le tableau RESOURCES avec filtres sp√©cifiques
    """
    try:
        logger.info("üìä Cr√©ation du tableau RESOURCES")
        
        resources_results = {}
        
        for file_type, df in dataframes.items():
            logger.info(f"üìÑ Traitement RESOURCES pour {file_type}")
            
            # V√©rification des colonnes requises
            resources_cols = ["Top Conso", "LCR_ECO_GROUPE_METIERS", "Sous-M√©tier", 
                            "Produit", "LCR_ECO_IMPACT_LCR"]
            missing_cols = [col for col in resources_cols if col not in df.columns]
            
            if missing_cols:
                logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour RESOURCES {file_type}: {missing_cols}")
                continue
            
            # Filtrage des donn√©es
            df_filtered = df[df["Top Conso"] == "O"].copy()
            
            # Filtres sp√©cifiques RESOURCES
            allowed_groupes = ["GLOBAL TRADE", "Other Contribution", "Treasury"]
            df_filtered = df_filtered[df_filtered["LCR_ECO_GROUPE_METIERS"].isin(allowed_groupes)].copy()
            
            excluded_sous_metier = ["GT GROUP SERVICES", "GT COMMODITY", "GT TRADE FINANCE", "SYN GLOBAL TRADE"]
            df_filtered = df_filtered[~df_filtered["Sous-M√©tier"].isin(excluded_sous_metier)].copy()
            
            excluded_produit = ["SIGHT DEPOSIT MIRROR", "SIGHT FINANCING MIRROR"]
            df_filtered = df_filtered[~df_filtered["Produit"].isin(excluded_produit)].copy()
            
            logger.info(f"üìã Apr√®s filtrage RESOURCES: {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e RESOURCES pour {file_type}")
                continue
            
            # Pr√©paration des donn√©es
            df_filtered["LCR_ECO_IMPACT_LCR"] = pd.to_numeric(
                df_filtered["LCR_ECO_IMPACT_LCR"], errors='coerce'
            ).fillna(0)
            
            # Groupement par LCR_ECO_GROUPE_METIERS
            grouped = df_filtered.groupby("LCR_ECO_GROUPE_METIERS")["LCR_ECO_IMPACT_LCR"].sum().reset_index()
            grouped["LCR_ECO_IMPACT_LCR_Bn"] = (grouped["LCR_ECO_IMPACT_LCR"] / 1_000_000_000).round(3)
            
            # Convertir en dictionnaire avec types Python natifs
            resources_results[file_type] = [
                {
                    "LCR_ECO_GROUPE_METIERS": str(row["LCR_ECO_GROUPE_METIERS"]),
                    "LCR_ECO_IMPACT_LCR": float(row["LCR_ECO_IMPACT_LCR"]),
                    "LCR_ECO_IMPACT_LCR_Bn": float(row["LCR_ECO_IMPACT_LCR_Bn"])
                }
                for _, row in grouped.iterrows()
            ]
            
            logger.info(f"‚úÖ RESOURCES {file_type}: {len(grouped)} groupes")
        
        return {
            "title": "RESOURCES",
            "data": resources_results,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filters_applied": {
                    "top_conso": "O",
                    "groupe_metiers": allowed_groupes,
                    "excluded_sous_metier": excluded_sous_metier,
                    "excluded_produit": excluded_produit
                }
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation tableau RESOURCES: {e}")
        return {
            "title": "RESOURCES - Erreur",
            "error": str(e)
        }
    

# ========================== FONCTIONS CAPPAGE TABLE ===========================


def create_cappage_table(dataframes):
    """
    Cr√©e le tableau CAPPAGE & Short_LCR avec structure pivot par date
    """
    try:
        logger.info("üìä Cr√©ation du tableau CAPPAGE & Short_LCR")
        
        cappage_results = {}
        
        for file_type, df in dataframes.items():
            logger.info(f"üìÑ Traitement CAPPAGE pour {file_type}")
            
            # V√©rification des colonnes requises
            cappage_cols = ["Top Conso", "SI Remettant", "Commentaire", 
                           "Date d'arr√™t√©", "LCR_Assiette Pond√©r√©e"]
            missing_cols = [col for col in cappage_cols if col not in df.columns]
            
            if missing_cols:
                logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour CAPPAGE {file_type}: {missing_cols}")
                continue
            
            # Filtrage des donn√©es
            df_filtered = df[df["Top Conso"] == "O"].copy()
            
            # Filtres sp√©cifiques CAPPAGE
            allowed_si_remettant = ["SHORT_LCR", "CAPREOS"]
            df_filtered = df_filtered[df_filtered["SI Remettant"].isin(allowed_si_remettant)].copy()
            
            logger.info(f"üìã Apr√®s filtrage CAPPAGE: {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e CAPPAGE pour {file_type}")
                continue
            
            # Pr√©paration des donn√©es
            df_filtered["LCR_Assiette Pond√©r√©e"] = pd.to_numeric(
                df_filtered["LCR_Assiette Pond√©r√©e"], errors='coerce'
            ).fillna(0)
            
            # Nettoyage des champs texte
            df_filtered["SI Remettant"] = df_filtered["SI Remettant"].astype(str).str.strip()
            df_filtered["Commentaire"] = df_filtered["Commentaire"].astype(str).str.strip()
            df_filtered["Date d'arr√™t√©"] = df_filtered["Date d'arr√™t√©"].astype(str).str.strip()
            
            # Structure de donn√©es pour le tableau crois√©
            cappage_data = []
            
            # Obtenir toutes les dates uniques
            dates = sorted(df_filtered["Date d'arr√™t√©"].unique())
            
            # Traitement par SI Remettant
            for si_remettant in allowed_si_remettant:
                si_data = df_filtered[df_filtered["SI Remettant"] == si_remettant]
                
                if si_remettant == "CAPREOS":
                    # Pour CAPREOS, montrer le d√©tail par Commentaire
                    for commentaire in si_data["Commentaire"].unique():
                        commentaire_data = si_data[si_data["Commentaire"] == commentaire]
                        
                        # Cr√©er une ligne pour chaque commentaire avec toutes les dates
                        row_data = {
                            "si_remettant": si_remettant,
                            "commentaire": commentaire,
                            "is_detail": True,
                            "dates": {}
                        }
                        
                        for date in dates:
                            date_data = commentaire_data[commentaire_data["Date d'arr√™t√©"] == date]
                            total = float(date_data["LCR_Assiette Pond√©r√©e"].sum()) / 1_000_000_000
                            row_data["dates"][date] = total
                        
                        cappage_data.append(row_data)
                else:
                    # Pour SHORT_LCR, montrer seulement le total
                    row_data = {
                        "si_remettant": si_remettant,
                        "commentaire": "TOTAL",
                        "is_detail": False,
                        "dates": {}
                    }
                    
                    for date in dates:
                        date_data = si_data[si_data["Date d'arr√™t√©"] == date]
                        total = float(date_data["LCR_Assiette Pond√©r√©e"].sum()) / 1_000_000_000
                        row_data["dates"][date] = total
                    
                    cappage_data.append(row_data)
            
            cappage_results[file_type] = {
                "data": cappage_data,
                "dates": dates
            }
            
            logger.info(f"‚úÖ CAPPAGE {file_type}: {len(cappage_data)} lignes, {len(dates)} dates")
        
        return {
            "title": "CAPPAGE & Short_LCR",
            "data": cappage_results,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filters_applied": {
                    "top_conso": "O",
                    "si_remettant": allowed_si_remettant
                }
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation tableau CAPPAGE: {e}")
        return {
            "title": "CAPPAGE & Short_LCR - Erreur",
            "error": str(e)
        }



# ========================== FONCTIONS BUFFER & NCO TABLE ===========================


def create_buffer_nco_table(dataframes):
    """
    Cr√©e les tableaux BUFFER & NCO avec structure pivot par date
    """
    try:
        logger.info("üìä Cr√©ation des tableaux BUFFER & NCO")
        
        buffer_nco_results = {}
        
        for file_type, df in dataframes.items():
            logger.info(f"üìÑ Traitement BUFFER & NCO pour {file_type}")
            
            # V√©rification des colonnes requises
            buffer_nco_cols = ["Top Conso", "LCR_Cat√©gorie", "LCR_Template Section 1", 
                              "Libell√© Client", "Date d'arr√™t√©", "LCR_Assiette Pond√©r√©e"]
            missing_cols = [col for col in buffer_nco_cols if col not in df.columns]
            
            if missing_cols:
                logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour BUFFER & NCO {file_type}: {missing_cols}")
                continue
            
            # Filtrage Top Conso pour les deux tableaux
            df_filtered = df[df["Top Conso"] == "O"].copy()
            
            logger.info(f"üìã Apr√®s filtrage Top Conso: {len(df_filtered)} lignes")
            
            if len(df_filtered) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e BUFFER & NCO pour {file_type}")
                continue
            
            # Pr√©paration des donn√©es
            df_filtered["LCR_Assiette Pond√©r√©e"] = pd.to_numeric(
                df_filtered["LCR_Assiette Pond√©r√©e"], errors='coerce'
            ).fillna(0)
            
            # Nettoyage des champs texte
            df_filtered["LCR_Cat√©gorie"] = df_filtered["LCR_Cat√©gorie"].astype(str).str.strip()
            df_filtered["LCR_Template Section 1"] = df_filtered["LCR_Template Section 1"].astype(str).str.strip()
            df_filtered["Libell√© Client"] = df_filtered["Libell√© Client"].astype(str).str.strip()
            df_filtered["Date d'arr√™t√©"] = df_filtered["Date d'arr√™t√©"].astype(str).str.strip()
            
            # Obtenir toutes les dates uniques
            dates = sorted(df_filtered["Date d'arr√™t√©"].unique())
            
            # TABLEAU 1: BUFFER (avec filtre LCR_Cat√©gorie = "1- Buffer")
            buffer_data = []
            df_buffer = df_filtered[df_filtered["LCR_Cat√©gorie"] == "1- Buffer"].copy()
            
            if len(df_buffer) > 0:
                # Grouper par LCR_Template Section 1 et Libell√© Client
                for section in df_buffer["LCR_Template Section 1"].unique():
                    section_data = df_buffer[df_buffer["LCR_Template Section 1"] == section]
                    
                    for client in section_data["Libell√© Client"].unique():
                        client_data = section_data[section_data["Libell√© Client"] == client]
                        
                        row_data = {
                            "lcr_template_section": section,
                            "libelle_client": client,
                            "dates": {}
                        }
                        
                        for date in dates:
                            date_data = client_data[client_data["Date d'arr√™t√©"] == date]
                            total = float(date_data["LCR_Assiette Pond√©r√©e"].sum()) / 1_000_000_000
                            row_data["dates"][date] = total
                        
                        buffer_data.append(row_data)
            
            # TABLEAU 2: NCO (sans filtre, group√© par LCR_Cat√©gorie)
            nco_data = []
            
            for categorie in df_filtered["LCR_Cat√©gorie"].unique():
                categorie_data = df_filtered[df_filtered["LCR_Cat√©gorie"] == categorie]
                
                row_data = {
                    "lcr_categorie": categorie,
                    "dates": {}
                }
                
                for date in dates:
                    date_data = categorie_data[categorie_data["Date d'arr√™t√©"] == date]
                    total = float(date_data["LCR_Assiette Pond√©r√©e"].sum()) / 1_000_000_000
                    row_data["dates"][date] = total
                
                nco_data.append(row_data)
            
            buffer_nco_results[file_type] = {
                "buffer_data": buffer_data,
                "nco_data": nco_data,
                "dates": dates
            }
            
            logger.info(f"‚úÖ BUFFER & NCO {file_type}: Buffer={len(buffer_data)} lignes, NCO={len(nco_data)} lignes, {len(dates)} dates")
        
        return {
            "title": "BUFFER & NCO",
            "data": buffer_nco_results,
            "metadata": {
                "analysis_date": datetime.now().isoformat(),
                "filters_applied": {
                    "top_conso": "O",
                    "buffer_filter": "LCR_Cat√©gorie = '1- Buffer'",
                    "nco_filter": "Aucun filtre suppl√©mentaire"
                }
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur cr√©ation tableaux BUFFER & NCO: {e}")
        return {
            "title": "BUFFER & NCO - Erreur",
            "error": str(e)
        }
    

# ========================== FONCTIONS CONTEXTE CHATBOT ===========================


def prepare_analysis_context() -> str:
    """
    Pr√©pare le contexte d√©taill√© depuis les donn√©es sauvegard√©es
    """
    context_parts = []
    
    # Contexte m√©tier de base
    context_parts.append("CONTEXTE M√âTIER:")
    context_parts.append("- Application d'analyse LCR (Liquidity Coverage Ratio) pour banque")
    context_parts.append("- Analyse Balance Sheet (ACTIF/PASSIF) en milliards d'euros")
    context_parts.append("- Analyse Consumption par groupes m√©tiers en milliards")
    context_parts.append("- Comparaison J vs J-1 (aujourd'hui vs hier)")
    
    # Donn√©es d'analyse si disponibles
    if chatbot_session.get("context_data"):
        data = chatbot_session["context_data"]
        
        context_parts.append(f"\nANALYSE EFFECTU√âE LE : {data.get('analysis_timestamp', 'Inconnue')}")
        
        # Balance Sheet
        if data.get("balance_sheet") and not data["balance_sheet"].get("error"):
            bs = data["balance_sheet"]
            context_parts.append("\n=== BALANCE SHEET RESULTS ===")
            context_parts.append(f"Titre: {bs.get('title', 'Balance Sheet')}")
            
            if bs.get("variations"):
                context_parts.append("Variations d√©taill√©es:")
                for category, var_data in bs["variations"].items():
                    context_parts.append(f"- {category}: D-1 = {var_data['j_minus_1']} Md‚Ç¨, D = {var_data['j']} Md‚Ç¨")
                    context_parts.append(f"  ‚Üí Variation = {var_data['variation']} Md‚Ç¨")
            
            if bs.get("summary"):
                context_parts.append(f"R√©sum√© ex√©cutif: {bs['summary']}")
        
        # Consumption
        if data.get("consumption") and not data["consumption"].get("error"):
            cons = data["consumption"]
            context_parts.append("\n=== CONSUMPTION LCR RESULTS ===")
            context_parts.append(f"Titre: {cons.get('title', 'Consumption Analysis')}")
            
            # Variation globale
            if cons.get("variations", {}).get("global"):
                global_var = cons["variations"]["global"]
                context_parts.append(f"Consumption total: D-1 = {global_var['j_minus_1']} Md, D = {global_var['j']} Md")
                context_parts.append(f"Variation globale = {global_var['variation']} Md")
            
            # Variations par groupe m√©tier
            if cons.get("variations", {}).get("by_groupe_metiers"):
                context_parts.append("\nVariations par groupe m√©tier:")
                for groupe, var_data in cons["variations"]["by_groupe_metiers"].items():
                    if abs(var_data["variation"]) > 0.01:  # Seulement les variations > 10M‚Ç¨
                        context_parts.append(f"- {groupe}: {var_data['variation']} Md (D-1: {var_data['j_minus_1']}, D: {var_data['j']})")
            
            # Analyses textuelles
            if cons.get("analysis_text"):
                context_parts.append(f"\nAnalyse principale: {cons['analysis_text']}")
            
            if cons.get("metier_detailed_analysis"):
                context_parts.append(f"Analyse d√©taill√©e: {cons['metier_detailed_analysis']}")
            
            # Groupes significatifs
            if cons.get("significant_groups"):
                context_parts.append(f"Groupes avec variations significatives: {', '.join(cons['significant_groups'])}")
        
        # Informations sur les fichiers source
        if data.get("raw_dataframes_info"):
            context_parts.append("\n=== FICHIERS SOURCE ===")
            for file_type, info in data["raw_dataframes_info"].items():
                context_parts.append(f"Fichier {file_type}: {info['shape'][0]} lignes, {info['shape'][1]} colonnes")
                context_parts.append(f"Colonnes: {', '.join(info['columns'])}")
                if info.get("sample_data"):
                    context_parts.append("√âchantillon de donn√©es:")
                    for i, row in enumerate(info["sample_data"][:2]):  # 2 premi√®res lignes
                        context_parts.append(f"  Ligne {i+1}: {str(row)[:200]}...")
    
    else:
        context_parts.append("\nAucune analyse disponible - les analyses doivent √™tre lanc√©es d'abord.")
    
    return "\n".join(context_parts)

def prepare_documents_context() -> str:
    """
    Pr√©pare le contexte depuis les documents upload√©s
    """
    if not chatbot_session["uploaded_documents"]:
        return ""
    
    context_parts = []
    for doc in chatbot_session["uploaded_documents"]:
        context_parts.append(f"Document: {doc['filename']}")
        context_parts.append(f"Contenu: {doc['content'][:2000]}...")  # Limiter √† 2000 chars
        context_parts.append("---")
    
    return "\n".join(context_parts)

def prepare_conversation_context() -> str:
    """
    Pr√©pare le contexte complet incluant analyses + documents + historique
    """
    context_parts = []
    
    # Contexte des analyses
    context_parts.append(prepare_analysis_context())
    
    # Documents upload√©s
    docs_context = prepare_documents_context()
    if docs_context:
        context_parts.append(f"\n\nContext Documents:\n{docs_context}")
    
    # Historique de conversation (derniers 10 messages pour √©viter de surcharger)
    if chatbot_session["messages"]:
        context_parts.append("\n\nHistory of conversation:")
        for msg in chatbot_session["messages"][-10:]:
            role = "Utilisateur" if msg["type"] == "user" else "Assistant"
            context_parts.append(f"{role}: {msg['message']}")
        context_parts.append("\n--- End of history of conversation ---")
    
    return "\n".join(context_parts)


if __name__ == "__main__":
    print("üöÄ Steering ALM Metrics - Version Templates")
    print("üìä Interface: http://localhost:8000")
    print("üìÅ Templates: templates/index.html")
    print("üé® Styles: static/js/main.js")
    print("‚èπÔ∏è  Ctrl+C pour arr√™ter")
    
    uvicorn.run(
    app,
    host="0.0.0.0",
    port=8000,
    reload=False,
    log_level="info",
    timeout_keep_alive=900,  # 15 minutes
    limit_max_requests=100,  # R√©duire pour forcer le recyclage
    workers=1  # Une seule instance pour √©viter la duplication m√©moire
    )